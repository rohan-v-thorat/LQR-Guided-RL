<!<!DOCTYPE html>
    <html lang="en">

    <head>
        <meta charset="utf-8">
        <title>[ICCMS 2025] LQR-Guided RL</title>
        <meta name="viewport" content="width=device-width, initial-scale=1">

        <link rel="stylesheet" href="static/css/academicons.min.css">
        <link rel="stylesheet" href="static/css/fontawesome.min.css">
        <script src="static/js/all.min.js"></script>
        <script src="static/js/bulma-carousel.min.js"></script>


        <link rel="stylesheet" href="static/css/style.css">
        <script src="static/js/script.js"></script>
        <!-- Adding latex equations support -->
        <script>
            MathJax = {
              tex: { inlineMath: [['$', '$'], ['\\(', '\\)']] },
              svg: { fontCache: 'global' }
            };
        </script>
        <script id="MathJax-script" async 
                src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
        </script>
    </head>

    <body>
        <nav class="navbar" role="navigation" aria-label="main navigation">
            <div class="navbar-brand">
                <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
                    <span aria-hidden="true"></span>
                    <span aria-hidden="true"></span>
                    <span aria-hidden="true"></span>
                </a>
            </div>
            <div class="navbar-menu">
                <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
                    <a class="navbar-item" href="https://github.com/rohan-v-thorat">
                        <span class="icon">
                            <i class="fas fa-home"></i>
                        </span>
                    </a>

                    <div class="navbar-item has-dropdown is-hoverable">
                        <a class="navbar-link">
                            More Research
                        </a>
                        <div class="navbar-dropdown">
                            <a class="navbar-item" href="https://sites.google.com/view/rajdip-nayek/research">
                                SMICR group
                            </a>
                        </div>
                    </div>
                </div>

            </div>
        </nav>

        <!-- HEADER -->
        <section class="hero">
            <div class="hero-body">
                <div class="container is-max-desktop">
                    <div class="columns is-centered">
                        <div class="column has-text-centered">
                            <h1 class="title is-1 is-size-3-mobile publication-title">Safe Reinforcement Learning-Based Vibration Control: Overcoming Training Risks with LQR Guidance</h1>
                            <h2 class="subtitle is-4 opacity-1" style="margin-top: 1rem;opacity:0.7">ICCMS 2025</h2>
                            <div class="is-size-5 publication-authors">
                                <span class="author-block">
                                    <a href="https://rohan-v-thorat.github.io/">Rohan Vitthal Thorat</a>,</span>
                                <span class="author-block">
                                    <a href="https://github.com/JuhiSingh001"> Juhi Singh</a>,</span>
                                <span class="author-block">
                                    <a href="https://sites.google.com/view/rajdip-nayek/">
                                        Rajdip Nayek</a>
                                </span>
                            </div>

                            <div class="is-size-5 publication-authors">
                                <span class="author-block">Indian Institute of Technology Delhi</span>
                            </div>

                            <div class="column has-text-centered">
                                <div class="publication-links">
                                    <!-- PDF Link. -->
                                    <span class="link-block">
                                        <a href="https://arxiv.org/pdf/2510.01269"
                                            class="external-link button is-normal is-rounded is-dark">
                                            <span class="icon">
                                                <i class="fas fa-file-pdf"></i>
                                            </span>
                                            <span>Paper</span>
                                        </a>
                                    </span>
                                    <!-- arXiv link -->
                                    <span class="link-block">
                                        <a href="https://arxiv.org/abs/2510.01269"
                                            class="external-link button is-normal is-rounded is-dark">
                                            <span class="icon">
                                                <i class="ai ai-arxiv"></i>
                                            </span>
                                            <span>arXiv</span>
                                        </a>
                                    </span>
                                    <!-- code repo link -->
                                    <span class="link-block">
                                        <a href="https://github.com/rohan-v-thorat/LQR-Guided-RL"
                                            class="external-link button is-normal is-rounded is-dark">
                                            <span class="icon">
                                                <i class="fab fa-github"></i>
                                            </span>
                                            <span>Code</span>
                                        </a>
                                    </span>
                                </div>

                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <!-- <section class="hero teaser is-light">
            <div class="hero-body">
                <div class="container">
                    <div id="teaser">
                        <img src="static/img/teaser_new_transparant.png"/>
                    </div>
                </div>
            </div>
        </section> -->
        <section class="section">
            <div class="container is-max-desktop is-mobile">
                <div class="columns is-centered">
                    <div class="column is-four-fifths">
                        <!-- <h1 class="title is-3">Overview</h1> -->
                        <!-- <h2 class="title is-5">Overview</h2> -->
                        <div class="content has-text-justified">
                        Structural vibration risks in infrastructure demand robust controllers. 
                        Model-based strategies like LQR effectively stabilize these systems, but 
                        require accurate modelsâ€”often impractical for real structures. RL offers a model-free 
                        alternative, but direct deployment is hazardous because agent exploration during its training can be damaging.
                            
                        </div>
                        <h1 class="title is-3">How can RL be trained safely on physical (true) systems?</h1>
                        <div class="content has-text-justified">
                            We present a framework blending LQR policy guidance with RL.

                            <ul>
                                <li>The LQR controller, designed from arbitrary model parameters, guides the RL agent during exploration, 
                                    ensuring safety without relying on perfect system identification.</li>
                                <li>RL policies learn directly on physical (true) systems using reward signals; LQR actions are linearly mixed for guidence.</li>    
                                <li>Experiments demonstrate that the blended controller reduces training-time vibration compared to pure RL, and ultimately surpasses LQR in vibration suppression after learning, all while maintaining low control effort.</li>
                            </ul>
                            
                        </div>
                        This method opens practical paths for safe, model-free vibration control in real-time applications where training risks are a major concern.
                        <br>
                        <div class="column is-centered" id="Fig1" align ="middle">
                            <img src="static/img/LQR-Guided-RL.svg" alt="LQR-Guided RL" style="width: 100%; max-width: 500px;"/>
                            <p class="caption has-text-centered">Figure 1: Overview of the LQR-Guided Reinforcement Learning framework: a classical spring-mass-damper system is controlled using a methodology that integrates Linear Quadratic Regulator (LQR) guidance within a reinforcement learning paradigm, leveraging both analytical control theory and data-driven learning.
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <section class="section">
            <div class="container is-max-desktop is-mobile">
                <!-- TL;DR. -->
                <div class="columns is-centered">
                    <div class="column is-four-fifths">
                        <h1 class="title is-3">Performance of LQR-Guided RL</h1>
                        A LQR is designed based on assumed linear dynamical system whose parameters value are chosen arbitrary.
                        This designed LQR is used to Guide the RL during its training and testing phase on a true system. The assumed system and true system (nonlinear)
                        considered are as below:
                        $$\text{Assumed system}\hspace{2em}1.6\ddot{x} - 0.5\dot{x} + 181x = u - 1.6\ddot{x}_{g}$$
                        $$ \text{True system}\hspace{3em}\ddot{x} + 0.4\dot{x} + 100x + x^3= u - \ddot{x}^g$$
                        where $x,u,\ddot{x}^g$ are the displacement, control force and ground acceleration respectively.
                        <br>
                        <br>
                        <h2 class="title is-5">Training phase</h2>
                            To assess the effectiveness of the proposed approach, the acceleration response 
                            of the dynamical system during the training phase is compared between the LQR-guided 
                            reinforcement learning (RL) policy and the non-guided RL policy. As shown in 
                            Figure <a href="#Fig2">[2]</a>, the incorporation of the LQR prior leads 
                            to a significantly reduced acceleration response relative to the non-guided RL policy. 
                        <div class="column is-centered" id="Fig2" align ="middle">
                            <img src="static/img/LQR_Guided_RL_training.png" alt="LQR-Guided RL" style="width: 100%; max-width: 500px;"/>
                            <p class="caption has-text-centered">Figure 2: Comparison of acceleration response during training of RL controller and LQR-Guided RL controller
                        </div>
                        <h2 class="title is-5">Testing phase</h2>
                            The LQR-guided RL policy and the standalone LQR policy are tested on the true 
                            nonlinear system. As illustrated in Figure <a href="#Fig3">[3]</a>, the 
                            LQR-guided RL policy exhibits lower acceleration responses and requires reduced control 
                            effort compared to the LQR policy. These results indicate that LQR guidance improves the 
                            safety in the training process of the RL policy, and also, the trained RL policy achieves 
                            superior performance compared to the standalone LQR policy.                        
                        <div class="column is-centered" id="Fig3" align ="middle">
                            <img src="static/img/LQR_Guided_RL_testing.png" alt="LQR-Guided RL" style="width: 100%; max-width: 500px;"/>
                            <p class="caption has-text-centered">Figure 3: Comparison of displacement, velocity, acceleration response and control force during testing for the case of LQR-Guided RL and LQR
                        </div>
                    </div>
                </div>
            </div>
        </section>


        <section class="section">
            <div class="container is-max-desktop is-mobile">
                <!-- Abstract. -->
                <div class="columns is-centered">
                    <div class="column is-four-fifths">
                        <h2 class="title is-3">Abstract</h2>
                        <div class="content has-text-justified">
                        Structural vibrations induced by external excitations pose significant risks, including safety hazards for occupants, structural damage, and increased maintenance costs. While conventional model-based control strategies like Linear Quadratic Regulator (LQR) and Model Predictive Control (MPC) effectively mitigate vibrations, their reliance on accurate system models necessitates tedious system identification. This tedious system identification process can be avoided by using model-free Reinforcement learning (RL) method for vibration control task. 
                        <br>
                        RL controllers derive their policies solely from observed structural behaviour, eliminating the requirement for an explicit structural model. For an RL controller to be truly model-free, its training must occur on the actual physical system rather than in simulation. However, during this training phase, the RL controller lacks prior knowledge and it exerts control force on the structure randomly, which 
                        can potentially harm the structure. To mitigate this risk, we propose guiding the RL controller using a Linear Quadratic Regulator (LQR) controller. While LQR control typically relies on an accurate structural model for optimal performance, our observations indicate that even an LQR controller based on an entirely incorrect model outperforms the uncontrolled scenario. Motivated by this finding, we 
                        introduce a hybrid control framework that integrates both LQR and RL controllers. In this approach, the LQR policy is derived from a randomly selected model and its parameters. As this LQR policy does not require knowledge of the true or an approximate structural model the overall framework remains model-free. This hybrid approach eliminates dependency on explicit system models while minimizing 
                        exploration risks inherent in naive RL implementations. As per our knowledge, this is the first study to address the critical training safety challenge of RL-based vibration control and provide a validated solution.
                        </div>
                    </div>
                </div>
            </div>
        </section>


        <section class="section" id="BibTeX">
            <div class="container is-max-desktop content">
                <h2 class="title">BibTeX</h2>
                <pre><code>
Not yet added
                </code></pre>
            </div>
        </section>


        <footer class="footer">
            <div class="container">
                <div class="content has-text-centered">
                    <a class="icon-link" href="https://arxiv.org/pdf/2510.01269">
                        <i class="fas fa-file-pdf"></i>
                    </a>
                    <a class="icon-link" href="https://github.com/rohan-v-thorat" class="external-link" disabled>
                        <i class="fab fa-github"></i>
                    </a>
                </div>
                <div class="columns is-centered">
                    <div class="column is-8">
                        <div class="content">
                            <p>
                                Website source code borrowed from <a
                                    href="https://github.com/kanishkbh/prdp-paper">Kanishk Bhatia github</a>.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </footer>
    </body>

    </html>
